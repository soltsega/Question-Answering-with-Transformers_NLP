{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase IV: Model Implementation & Fine-Tuning\n",
    "\n",
    "This notebook implements the complete training pipeline for fine-tuning DistilBERT on the SQuAD v1.1 dataset for question answering.\n",
    "\n",
    "**Based on Previous Phases:**\n",
    "- **Phase II EDA**: Optimized parameters (max_length=384, doc_stride=128)\n",
    "- **Phase III Preprocessing**: Validated tokenization pipeline\n",
    "- **Model Choice**: DistilBERT for speed/accuracy balance\n",
    "\n",
    "**Training Strategy:**\n",
    "- Use Hugging Face Trainer API for efficient training\n",
    "- Implement proper validation and metrics tracking\n",
    "- Save checkpoints and monitor training progress\n",
    "- Optimize hyperparameters based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\My Device\\Desktop\\Question Answering with Transformers_NLP\\localenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Phase IV: Model Implementation & Fine-Tuning\n",
    "\n",
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from src.preprocessing import prepare_train_features\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection & Setup\n",
    "\n",
    "Load DistilBERT model and tokenizer with optimized configuration from Phase III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\My Device\\Desktop\\Question Answering with Transformers_NLP\\localenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\My Device\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 221.44it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForQuestionAnswering LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "qa_outputs.weight       | MISSING    | \n",
      "qa_outputs.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased\n",
      "Model parameters: 66,364,418\n",
      "Model trainable parameters: 66,364,418\n",
      "Max sequence length: 384\n",
      "Document stride: 128\n"
     ]
    }
   ],
   "source": [
    "# Model configuration based on Phase III findings\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 384  # From Phase III validation\n",
    "DOC_STRIDE = 128  # From Phase III optimization\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Document stride: {DOC_STRIDE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing\n",
    "\n",
    "Load SQuAD dataset and apply the validated preprocessing pipeline from Phase III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD v1.1 dataset...\n",
      "Train samples: 87599\n",
      "Validation samples: 10570\n",
      "\n",
      "Using subsets:\n",
      "Train: 1000 samples\n",
      "Validation: 200 samples\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading SQuAD v1.1 dataset...\")\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "\n",
    "print(f\"Train samples: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation samples: {len(raw_datasets['validation'])}\")\n",
    "\n",
    "# For initial training, use a subset to verify everything works\n",
    "TRAIN_SUBSET_SIZE = 1000  # Start with smaller subset for testing\n",
    "VAL_SUBSET_SIZE = 200    # Smaller validation set\n",
    "\n",
    "train_dataset = raw_datasets['train'].select(range(TRAIN_SUBSET_SIZE))\n",
    "val_dataset = raw_datasets['validation'].select(range(VAL_SUBSET_SIZE))\n",
    "\n",
    "print(f\"\\nUsing subsets:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Training features: 1032\n",
      "Preprocessing validation data...\n",
      "Validation features: 200\n",
      "\n",
      "Preprocessing complete!\n",
      "Train expansion ratio: 1.03x\n",
      "Val expansion ratio: 1.00x\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing function\n",
    "def preprocess_dataset(dataset, tokenizer, max_length=MAX_LENGTH, doc_stride=DOC_STRIDE):\n",
    "    \"\"\"Apply preprocessing to dataset with progress tracking.\"\"\"\n",
    "    # Convert to format expected by preprocessing function\n",
    "    dataset_dict = {\n",
    "        \"question\": [example[\"question\"] for example in dataset],\n",
    "        \"context\": [example[\"context\"] for example in dataset],\n",
    "        \"answers\": [example[\"answers\"] for example in dataset]\n",
    "    }\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed = prepare_train_features(dataset_dict, tokenizer, max_length, doc_stride)\n",
    "    \n",
    "    # Convert to torch format\n",
    "    processed_features = {\n",
    "        'input_ids': torch.tensor(processed['input_ids']),\n",
    "        'attention_mask': torch.tensor(processed['attention_mask']),\n",
    "        'start_positions': torch.tensor(processed['start_positions']),\n",
    "        'end_positions': torch.tensor(processed['end_positions'])\n",
    "    }\n",
    "    \n",
    "    return processed_features\n",
    "\n",
    "# Preprocess datasets\n",
    "print(\"Preprocessing training data...\")\n",
    "train_processed = preprocess_dataset(train_dataset, tokenizer)\n",
    "print(f\"Training features: {len(train_processed['input_ids'])}\")\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_processed = preprocess_dataset(val_dataset, tokenizer)\n",
    "print(f\"Validation features: {len(val_processed['input_ids'])}\")\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n",
    "print(f\"Train expansion ratio: {len(train_processed['input_ids'])/len(train_dataset):.2f}x\")\n",
    "print(f\"Val expansion ratio: {len(val_processed['input_ids'])/len(val_dataset):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure training arguments based on our dataset characteristics and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Configuration ===\n",
      "Batch size: 8\n",
      "Learning rate: 3e-05\n",
      "Epochs: 3\n",
      "Weight decay: 0.01\n",
      "Warmup steps: 100\n",
      "Total training steps: 387\n",
      "\n",
      "Estimated memory per batch: 0.0 MB\n",
      "Recommended GPU memory: 0.1 MB (with gradients)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 8  # Adjust based on GPU memory\n",
    "LEARNING_RATE = 3e-5  # Standard for transformer fine-tuning\n",
    "NUM_EPOCHS = 3  # Start with 3 epochs for initial training\n",
    "WEIGHT_DECAY = 0.01  # Prevent overfitting\n",
    "WARMUP_STEPS = 100  # Gradual learning rate warmup\n",
    "\n",
    "# Calculate training steps\n",
    "TOTAL_TRAIN_STEPS = (len(train_processed['input_ids']) // BATCH_SIZE) * NUM_EPOCHS\n",
    "\n",
    "print(\"=== Training Configuration ===\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"Total training steps: {TOTAL_TRAIN_STEPS}\")\n",
    "\n",
    "# Memory estimate\n",
    "memory_per_batch = BATCH_SIZE * MAX_LENGTH * 4 * 3  # input_ids, attention_mask, labels\n",
    "print(f\"\\nEstimated memory per batch: {memory_per_batch / 1024**2:.1f} MB\")\n",
    "print(f\"Recommended GPU memory: {memory_per_batch / 1024**2 * 4:.1f} MB (with gradients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Setup training arguments\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/checkpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable wandb/tensorboard for now\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use mixed precision if GPU available\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save memory\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep all columns for QA task\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining arguments configured:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Output directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/checkpoints\",\n",
    "    eval_strategy=\"epoch\",  # Fixed: evaluation_strategy -> eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard for now\n",
    "    dataloader_pin_memory=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    remove_unused_columns=False  # Keep all columns for QA task\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")\n",
    "print(f\"  Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\"  Save strategy: {training_args.save_strategy}\")\n",
    "print(f\"  Mixed precision: {training_args.fp16}\")\n",
    "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collator & Trainer Setup\n",
    "\n",
    "Setup data collator for padding and initialize the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class for our processed data\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.features['input_ids'][idx],\n",
    "            'attention_mask': self.features['attention_mask'][idx],\n",
    "            'start_positions': self.features['start_positions'][idx],\n",
    "            'end_positions': self.features['end_positions'][idx]\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset_obj = QADataset(train_processed)\n",
    "val_dataset_obj = QADataset(val_processed)\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset_obj)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_obj)}\")\n",
    "print(f\"Data collator configured with tokenizer: {tokenizer.name_or_path}\")\n",
    "\n",
    "# Test a sample batch\n",
    "sample_batch = [train_dataset_obj[0], train_dataset_obj[1]]\n",
    "collated_batch = data_collator(sample_batch)\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "for key, value in collated_batch.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_obj,\n",
    "    eval_dataset=val_dataset_obj,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Model: {trainer.model.__class__.__name__}\")\n",
    "print(f\"Training samples: {len(trainer.train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(trainer.eval_dataset)}\")\n",
    "print(f\"Total training steps: {trainer.args.num_train_epochs * len(trainer.train_dataset) // trainer.args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Start the fine-tuning process with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting model training...\")\n",
    "print(f\"Configuration: {NUM_EPOCHS} epochs, batch size {BATCH_SIZE}, learning rate {LEARNING_RATE}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training samples: {len(trainer.train_dataset)}\")\n",
    "print(f\"Validation samples: {len(trainer.eval_dataset)}\")\n",
    "\n",
    "# Train the model\n",
    "training_results = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final training loss: {training_results.training_loss:.4f}\")\n",
    "print(f\"Total training time: {training_results.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples per second: {training_results.metrics['train_samples_per_second']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Results Analysis\n",
    "\n",
    "Analyze training progress and save the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training logs\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "# Extract training and validation losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "epochs = []\n",
    "\n",
    "for log in training_logs:\n",
    "    if 'train_loss' in log:\n",
    "        train_losses.append(log['train_loss'])\n",
    "        epochs.append(log['epoch'])\n",
    "    if 'eval_loss' in log:\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "if eval_losses:\n",
    "    eval_epochs = list(range(1, len(eval_losses) + 1))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_epochs, eval_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Training Summary ===\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "if eval_losses:\n",
    "    print(f\"Final validation loss: {eval_losses[-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(eval_losses):.4f}\")\n",
    "print(f\"Total epochs trained: {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = \"../models/distilbert-squad-finetuned\"\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"doc_stride\": DOC_STRIDE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"total_train_steps\": TOTAL_TRAIN_STEPS,\n",
    "    \"final_train_loss\": training_results.training_loss,\n",
    "    \"training_time_seconds\": training_results.metrics['train_runtime'],\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "with open(f\"{final_model_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Tokenizer saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Training config saved to: {final_model_path}/training_config.json\")\n",
    "\n",
    "# Model size\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel statistics:\")\n",
    "print(f\"  Total parameters: {model_size:,}\")\n",
    "print(f\"  Model size: {model_size * 4 / 1024**2:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Validation & Testing\n",
    "\n",
    "Test the trained model on sample questions to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for testing\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=final_model_path,\n",
    "    tokenizer=final_model_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ QA pipeline created successfully!\")\n",
    "\n",
    "# Test on sample examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"context\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\",\n",
    "        \"question\": \"Who is the Eiffel Tower named after?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass-energy equivalence formula E = mc¬≤, which has been dubbed 'the world's most famous equation'.\",\n",
    "        \"question\": \"What is Einstein most famous for?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"The Great Barrier Reef is the world's largest coral reef system composed of over 2,900 individual reefs and 900 islands stretching for over 2,300 kilometers over an area of approximately 344,400 square kilometers. The reef is located in the Coral Sea, off the coast of Queensland, Australia.\",\n",
    "        \"question\": \"Where is the Great Barrier Reef located?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n=== Testing Trained Model ===\")\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    result = qa_pipeline(question=example[\"question\"], context=example[\"context\"])\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary & Next Steps\n",
    "\n",
    "Summarize the training results and outline next steps for Phase V evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"phase\": \"IV - Model Implementation & Fine-Tuning\",\n",
    "    \"model\": {\n",
    "        \"base_model\": MODEL_NAME,\n",
    "        \"fine_tuned_model\": final_model_path,\n",
    "        \"parameters\": model_size,\n",
    "        \"model_size_mb\": model_size * 4 / 1024**2\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"val_samples\": len(val_dataset),\n",
    "        \"train_features\": len(train_processed['input_ids']),\n",
    "        \"val_features\": len(val_processed['input_ids']),\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"final_train_loss\": training_results.training_loss,\n",
    "        \"training_time_seconds\": training_results.metrics['train_runtime']\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"device\": str(device),\n",
    "        \"mixed_precision\": training_args.fp16\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"model_saved\": True,\n",
    "        \"pipeline_tested\": True,\n",
    "        \"ready_for_evaluation\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{final_model_path}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(\"=== Phase IV Training Complete ===\")\n",
    "print(f\"‚úÖ Model: {MODEL_NAME} fine-tuned on SQuAD v1.1\")\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"‚úÖ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"‚úÖ Final loss: {training_results.training_loss:.4f}\")\n",
    "print(f\"‚úÖ Model saved: {final_model_path}\")\n",
    "print(f\"‚úÖ Training time: {training_results.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "\n",
    "print(f\"\\nüìä Model ready for Phase V: Evaluation & Error Analysis\")\n",
    "print(f\"üìÅ Model files: {final_model_path}\")\n",
    "print(f\"üìÅ Training logs: {training_args.output_dir}\")\n",
    "print(f\"üìÅ Configuration: {final_model_path}/training_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
