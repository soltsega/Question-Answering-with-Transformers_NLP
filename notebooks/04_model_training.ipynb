{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase IV: Model Implementation & Fine-Tuning\n",
    "\n",
    "This notebook implements the complete training pipeline for fine-tuning DistilBERT on the SQuAD v1.1 dataset for question answering.\n",
    "\n",
    "**Based on Previous Phases:**\n",
    "- **Phase II EDA**: Optimized parameters (max_length=384, doc_stride=128)\n",
    "- **Phase III Preprocessing**: Validated tokenization pipeline\n",
    "- **Model Choice**: DistilBERT for speed/accuracy balance\n",
    "\n",
    "**Training Strategy:**\n",
    "- Use Hugging Face Trainer API for efficient training\n",
    "- Implement proper validation and metrics tracking\n",
    "- Save checkpoints and monitor training progress\n",
    "- Optimize hyperparameters based on validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing accelerate...\n",
      "‚úÖ Accelerate installed: 1.12.0\n",
      "Libraries imported successfully!\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Phase IV: Model Implementation & Fine-Tuning\n",
    "\n",
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Install accelerate if not available\n",
    "try:\n",
    "    import accelerate\n",
    "    print(f\"‚úÖ Accelerate already installed: {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing accelerate...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"accelerate>=1.1.0\"])\n",
    "    import accelerate\n",
    "    print(f\"‚úÖ Accelerate installed: {accelerate.__version__}\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from src.preprocessing import prepare_train_features\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection & Setup\n",
    "\n",
    "Load DistilBERT model and tokenizer with optimized configuration from Phase III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 184.75it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertForQuestionAnswering LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "qa_outputs.weight       | MISSING    | \n",
      "qa_outputs.bias         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased\n",
      "Model parameters: 66,364,418\n",
      "Model trainable parameters: 66,364,418\n",
      "Max sequence length: 384\n",
      "Document stride: 128\n"
     ]
    }
   ],
   "source": [
    "# Model configuration based on Phase III findings\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 384  # From Phase III validation\n",
    "DOC_STRIDE = 128  # From Phase III optimization\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Document stride: {DOC_STRIDE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing\n",
    "\n",
    "Load SQuAD dataset and apply the validated preprocessing pipeline from Phase III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD v1.1 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 87599\n",
      "Validation samples: 10570\n",
      "\n",
      "Using subsets:\n",
      "Train: 1000 samples\n",
      "Validation: 200 samples\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading SQuAD v1.1 dataset...\")\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "\n",
    "print(f\"Train samples: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation samples: {len(raw_datasets['validation'])}\")\n",
    "\n",
    "# For initial training, use a subset to verify everything works\n",
    "TRAIN_SUBSET_SIZE = 1000  # Start with smaller subset for testing\n",
    "VAL_SUBSET_SIZE = 200    # Smaller validation set\n",
    "\n",
    "train_dataset = raw_datasets['train'].select(range(TRAIN_SUBSET_SIZE))\n",
    "val_dataset = raw_datasets['validation'].select(range(VAL_SUBSET_SIZE))\n",
    "\n",
    "print(f\"\\nUsing subsets:\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Training features: 1032\n",
      "Preprocessing validation data...\n",
      "Validation features: 200\n",
      "\n",
      "Preprocessing complete!\n",
      "Train expansion ratio: 1.03x\n",
      "Val expansion ratio: 1.00x\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing function\n",
    "def preprocess_dataset(dataset, tokenizer, max_length=MAX_LENGTH, doc_stride=DOC_STRIDE):\n",
    "    \"\"\"Apply preprocessing to dataset with progress tracking.\"\"\"\n",
    "    # Convert to format expected by preprocessing function\n",
    "    dataset_dict = {\n",
    "        \"question\": [example[\"question\"] for example in dataset],\n",
    "        \"context\": [example[\"context\"] for example in dataset],\n",
    "        \"answers\": [example[\"answers\"] for example in dataset]\n",
    "    }\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed = prepare_train_features(dataset_dict, tokenizer, max_length, doc_stride)\n",
    "    \n",
    "    # Convert to torch format\n",
    "    processed_features = {\n",
    "        'input_ids': torch.tensor(processed['input_ids']),\n",
    "        'attention_mask': torch.tensor(processed['attention_mask']),\n",
    "        'start_positions': torch.tensor(processed['start_positions']),\n",
    "        'end_positions': torch.tensor(processed['end_positions'])\n",
    "    }\n",
    "    \n",
    "    return processed_features\n",
    "\n",
    "# Preprocess datasets\n",
    "print(\"Preprocessing training data...\")\n",
    "train_processed = preprocess_dataset(train_dataset, tokenizer)\n",
    "print(f\"Training features: {len(train_processed['input_ids'])}\")\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_processed = preprocess_dataset(val_dataset, tokenizer)\n",
    "print(f\"Validation features: {len(val_processed['input_ids'])}\")\n",
    "\n",
    "print(f\"\\nPreprocessing complete!\")\n",
    "print(f\"Train expansion ratio: {len(train_processed['input_ids'])/len(train_dataset):.2f}x\")\n",
    "print(f\"Val expansion ratio: {len(val_processed['input_ids'])/len(val_dataset):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure training arguments based on our dataset characteristics and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Configuration ===\n",
      "Batch size: 8\n",
      "Learning rate: 3e-05\n",
      "Epochs: 3\n",
      "Weight decay: 0.01\n",
      "Warmup steps: 100\n",
      "Total training steps: 387\n",
      "\n",
      "Estimated memory per batch: 0.0 MB\n",
      "Recommended GPU memory: 0.1 MB (with gradients)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 8  # Adjust based on GPU memory\n",
    "LEARNING_RATE = 3e-5  # Standard for transformer fine-tuning\n",
    "NUM_EPOCHS = 3  # Start with 3 epochs for initial training\n",
    "WEIGHT_DECAY = 0.01  # Prevent overfitting\n",
    "WARMUP_STEPS = 100  # Gradual learning rate warmup\n",
    "\n",
    "# Calculate training steps\n",
    "TOTAL_TRAIN_STEPS = (len(train_processed['input_ids']) // BATCH_SIZE) * NUM_EPOCHS\n",
    "\n",
    "print(\"=== Training Configuration ===\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"Total training steps: {TOTAL_TRAIN_STEPS}\")\n",
    "\n",
    "# Memory estimate\n",
    "memory_per_batch = BATCH_SIZE * MAX_LENGTH * 4 * 3  # input_ids, attention_mask, labels\n",
    "print(f\"\\nEstimated memory per batch: {memory_per_batch / 1024**2:.1f} MB\")\n",
    "print(f\"Recommended GPU memory: {memory_per_batch / 1024**2 * 4:.1f} MB (with gradients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=1.1.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=1.1.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Setup training arguments\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/checkpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fixed: evaluation_strategy -> eval_strategy\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# logging_dir=\"../logs\",  # Removed: deprecated parameter\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable wandb/tensorboard for now\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use mixed precision if GPU available\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save memory\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep all columns for QA task\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining arguments configured:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Output directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_args.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:112\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, per_device_train_batch_size, num_train_epochs, max_steps, learning_rate, lr_scheduler_type, lr_scheduler_kwargs, warmup_steps, optim, optim_args, weight_decay, adam_beta1, adam_beta2, adam_epsilon, optim_target_modules, gradient_accumulation_steps, average_tokens_across_devices, max_grad_norm, label_smoothing_factor, bf16, fp16, bf16_full_eval, fp16_full_eval, tf32, gradient_checkpointing, gradient_checkpointing_kwargs, torch_compile, torch_compile_backend, torch_compile_mode, use_liger_kernel, liger_kernel_config, use_cache, neftune_noise_alpha, torch_empty_cache_steps, auto_find_batch_size, logging_strategy, logging_steps, logging_first_step, log_on_each_node, logging_nan_inf_filter, include_num_input_tokens_seen, log_level, log_level_replica, disable_tqdm, report_to, run_name, project, trackio_space_id, eval_strategy, eval_steps, eval_delay, per_device_eval_batch_size, prediction_loss_only, eval_on_start, eval_do_concat_batches, eval_use_gather_object, eval_accumulation_steps, include_for_metrics, batch_eval_metrics, save_only_model, save_strategy, save_steps, save_on_each_node, save_total_limit, enable_jit_checkpoint, push_to_hub, hub_token, hub_private_repo, hub_model_id, hub_strategy, hub_always_push, hub_revision, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, restore_callback_states_from_checkpoint, full_determinism, seed, data_seed, use_cpu, accelerator_config, parallelism_config, dataloader_drop_last, dataloader_num_workers, dataloader_pin_memory, dataloader_persistent_workers, dataloader_prefetch_factor, remove_unused_columns, label_names, train_sampling_strategy, length_column_name, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, ddp_backend, ddp_timeout, fsdp, fsdp_config, deepspeed, debug, skip_memory_metrics, do_train, do_eval, do_predict, resume_from_checkpoint, warmup_ratio, logging_dir, local_rank)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Question Answering with Transformers_NLP\\localenv\\Lib\\site-packages\\transformers\\training_args.py:1573\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1571\u001b[39m \u001b[38;5;66;03m# ‚îÄ‚îÄ 8. Device Init ‚îÄ‚îÄ\u001b[39;00m\n\u001b[32m   1572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1573\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[38;5;66;03m# ‚îÄ‚îÄ 9. TF32 ‚îÄ‚îÄ\u001b[39;00m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torch_compile:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Question Answering with Transformers_NLP\\localenv\\Lib\\site-packages\\transformers\\training_args.py:1853\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1850\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   1851\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1852\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1853\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:1001\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    999\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1003\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Question Answering with Transformers_NLP\\localenv\\Lib\\site-packages\\transformers\\training_args.py:1748\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   1749\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1750\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1751\u001b[39m         )\n\u001b[32m   1752\u001b[39m \u001b[38;5;66;03m# Build kwargs for PartialState; actual init happens below\u001b[39;00m\n\u001b[32m   1753\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=1.1.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=1.1.0'`"
     ]
    }
   ],
   "source": [
    "# Alternative: Pure PyTorch Training Loop (bypasses accelerate/Trainer)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "import time  # Added missing import\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üîß Using Pure PyTorch Training Loop (no accelerate dependency)\")\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'start_positions': torch.stack([item['start_positions'] for item in batch]),\n",
    "        'end_positions': torch.stack([item['end_positions'] for item in batch])\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset_obj, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset_obj, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training setup complete:\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Mixed precision: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up complete PyTorch training pipeline...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollate_fn\u001b[39m(batch):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     22\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m: torch.stack([item[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]),\n\u001b[32m     23\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m: torch.stack([item[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]),\n\u001b[32m     24\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstart_positions\u001b[39m\u001b[33m'\u001b[39m: torch.stack([item[\u001b[33m'\u001b[39m\u001b[33mstart_positions\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]),\n\u001b[32m     25\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mend_positions\u001b[39m\u001b[33m'\u001b[39m: torch.stack([item[\u001b[33m'\u001b[39m\u001b[33mend_positions\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[32m     26\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m train_loader = DataLoader(\u001b[43mtrain_dataset_obj\u001b[49m, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate_fn)\n\u001b[32m     29\u001b[39m val_loader = DataLoader(val_dataset_obj, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn=collate_fn)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Setup optimizer (using PyTorch's AdamW)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset_obj' is not defined"
     ]
    }
   ],
   "source": [
    "# Complete PyTorch Training Setup and Execution\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üîß Setting up complete PyTorch training pipeline...\")\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Create dataset objects (from previous cells)\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.features['input_ids'][idx],\n",
    "            'attention_mask': self.features['attention_mask'][idx],\n",
    "            'start_positions': self.features['start_positions'][idx],\n",
    "            'end_positions': self.features['end_positions'][idx]\n",
    "        }\n",
    "\n",
    "# Recreate dataset objects\n",
    "train_dataset_obj = QADataset(train_processed)\n",
    "val_dataset_obj = QADataset(val_processed)\n",
    "\n",
    "# Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'start_positions': torch.stack([item['start_positions'] for item in batch]),\n",
    "        'end_positions': torch.stack([item['end_positions'] for item in batch])\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset_obj, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset_obj, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Setup optimizer (using PyTorch's AdamW)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Training setup complete:\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Mixed precision: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    \"\"\"Train model using pure PyTorch (no Trainer/accelerate)\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Mixed precision setup\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "    \n",
    "    print(\"üöÄ Starting PyTorch training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_train_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Training loop\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision if available\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions\n",
    "                    )\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                start_positions = batch['start_positions'].to(device)\n",
    "                end_positions = batch['end_positions'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Time: {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training completed in {total_time:.1f} seconds\")\n",
    "    \n",
    "    return train_losses, val_losses, total_time\n",
    "\n",
    "# Start training\n",
    "train_losses, val_losses, training_time = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Training Summary ===\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {min(val_losses):.4f}\")\n",
    "print(f\"Total training time: {training_time:.1f} seconds\")\n",
    "print(f\"Epochs completed: {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model (PyTorch way)\n",
    "final_model_path = \"../models/distilbert-squad-finetuned-pytorch\"\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"doc_stride\": DOC_STRIDE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"final_train_loss\": train_losses[-1],\n",
    "    \"final_val_loss\": val_losses[-1],\n",
    "    \"training_time_seconds\": training_time,\n",
    "    \"device\": str(device),\n",
    "    \"training_method\": \"pure_pytorch\"\n",
    "}\n",
    "\n",
    "with open(f\"{final_model_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Tokenizer saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Training config saved to: {final_model_path}/training_config.json\")\n",
    "\n",
    "# Model size\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel statistics:\")\n",
    "print(f\"  Total parameters: {model_size:,}\")\n",
    "print(f\"  Model size: {model_size * 4 / 1024**2:.1f} MB (float32)\")\n",
    "print(f\"  Training method: Pure PyTorch (no accelerate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Start the fine-tuning process with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting model training...\")\n",
    "print(f\"Configuration: {NUM_EPOCHS} epochs, batch size {BATCH_SIZE}, learning rate {LEARNING_RATE}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training samples: {len(trainer.train_dataset)}\")\n",
    "print(f\"Validation samples: {len(trainer.eval_dataset)}\")\n",
    "\n",
    "# Train the model\n",
    "training_results = trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final training loss: {training_results.training_loss:.4f}\")\n",
    "print(f\"Total training time: {training_results.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training samples per second: {training_results.metrics['train_samples_per_second']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Results Analysis\n",
    "\n",
    "Analyze training progress and save the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training logs\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "# Extract training and validation losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "epochs = []\n",
    "\n",
    "for log in training_logs:\n",
    "    if 'train_loss' in log:\n",
    "        train_losses.append(log['train_loss'])\n",
    "        epochs.append(log['epoch'])\n",
    "    if 'eval_loss' in log:\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "if eval_losses:\n",
    "    eval_epochs = list(range(1, len(eval_losses) + 1))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(eval_epochs, eval_losses, 'r-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== Training Summary ===\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "if eval_losses:\n",
    "    print(f\"Final validation loss: {eval_losses[-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(eval_losses):.4f}\")\n",
    "print(f\"Total epochs trained: {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = \"../models/distilbert-squad-finetuned\"\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"doc_stride\": DOC_STRIDE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"total_train_steps\": TOTAL_TRAIN_STEPS,\n",
    "    \"final_train_loss\": training_results.training_loss,\n",
    "    \"training_time_seconds\": training_results.metrics['train_runtime'],\n",
    "    \"device\": str(device)\n",
    "}\n",
    "\n",
    "with open(f\"{final_model_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Tokenizer saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Training config saved to: {final_model_path}/training_config.json\")\n",
    "\n",
    "# Model size\n",
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel statistics:\")\n",
    "print(f\"  Total parameters: {model_size:,}\")\n",
    "print(f\"  Model size: {model_size * 4 / 1024**2:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Validation & Testing\n",
    "\n",
    "Test the trained model on sample questions to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for testing\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=final_model_path,\n",
    "    tokenizer=final_model_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ QA pipeline created successfully!\")\n",
    "\n",
    "# Test on sample examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"context\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\",\n",
    "        \"question\": \"Who is the Eiffel Tower named after?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass-energy equivalence formula E = mc¬≤, which has been dubbed 'the world's most famous equation'.\",\n",
    "        \"question\": \"What is Einstein most famous for?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"The Great Barrier Reef is the world's largest coral reef system composed of over 2,900 individual reefs and 900 islands stretching for over 2,300 kilometers over an area of approximately 344,400 square kilometers. The reef is located in the Coral Sea, off the coast of Queensland, Australia.\",\n",
    "        \"question\": \"Where is the Great Barrier Reef located?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n=== Testing Trained Model ===\")\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    result = qa_pipeline(question=example[\"question\"], context=example[\"context\"])\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Summary & Next Steps\n",
    "\n",
    "Summarize the training results and outline next steps for Phase V evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"phase\": \"IV - Model Implementation & Fine-Tuning\",\n",
    "    \"model\": {\n",
    "        \"base_model\": MODEL_NAME,\n",
    "        \"fine_tuned_model\": final_model_path,\n",
    "        \"parameters\": model_size,\n",
    "        \"model_size_mb\": model_size * 4 / 1024**2\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"val_samples\": len(val_dataset),\n",
    "        \"train_features\": len(train_processed['input_ids']),\n",
    "        \"val_features\": len(val_processed['input_ids']),\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"final_train_loss\": training_results.training_loss,\n",
    "        \"training_time_seconds\": training_results.metrics['train_runtime']\n",
    "    },\n",
    "    \"hardware\": {\n",
    "        \"device\": str(device),\n",
    "        \"mixed_precision\": training_args.fp16\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"model_saved\": True,\n",
    "        \"pipeline_tested\": True,\n",
    "        \"ready_for_evaluation\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{final_model_path}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(\"=== Phase IV Training Complete ===\")\n",
    "print(f\"‚úÖ Model: {MODEL_NAME} fine-tuned on SQuAD v1.1\")\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"‚úÖ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"‚úÖ Final loss: {training_results.training_loss:.4f}\")\n",
    "print(f\"‚úÖ Model saved: {final_model_path}\")\n",
    "print(f\"‚úÖ Training time: {training_results.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "\n",
    "print(f\"\\nüìä Model ready for Phase V: Evaluation & Error Analysis\")\n",
    "print(f\"üìÅ Model files: {final_model_path}\")\n",
    "print(f\"üìÅ Training logs: {training_args.output_dir}\")\n",
    "print(f\"üìÅ Configuration: {final_model_path}/training_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
