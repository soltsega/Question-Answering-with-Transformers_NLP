{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase II: Preprocessing and Tokenization\n",
                "\n",
                "This notebook demonstrates the preprocessing pipeline for converting SQuAD raw text into tokenized features with start and end position labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\My Device\\Desktop\\Question Answering with Transformers_NLP\\localenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "from src.preprocessing import get_tokenizer, prepare_train_features\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = load_dataset(\"squad\", split=\"train[:10]\")\n",
                "tokenizer = get_tokenizer()\n",
                "print(f\"Loaded {len(dataset)} samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Apply Preprocessing\n",
                "\n",
                "We use the `prepare_train_features` function which handles:\n",
                "- Truncation of long contexts.\n",
                "- Mapping character-based answer start/end to token-based positions.\n",
                "- Handling of the sliding window (stride) approach."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = prepare_train_features(dataset, tokenizer)\n",
                "print(f\"Generated {len(features['input_ids'])} features from {len(dataset)} samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Verify Results\n",
                "\n",
                "Let's decode the predicted spans and compare them with the original answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "for i in range(min(5, len(features['input_ids']))):\n",
                "    start = features['start_positions'][i]\n",
                "    end = features['end_positions'][i]\n",
                "    input_ids = features['input_ids'][i]\n",
                "    \n",
                "    decoded_answer = tokenizer.decode(input_ids[start:end+1])\n",
                "    \n",
                "    results.append({\n",
                "        \"Feature Index\": i,\n",
                "        \"Start Position\": start,\n",
                "        \"End Position\": end,\n",
                "        \"Decoded Span\": decoded_answer\n",
                "    })\n",
                "\n",
                "pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Preprocessed Sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "os.makedirs(\"data\", exist_ok=True)\n",
                "sample_feature = {\n",
                "    \"input_ids\": features[\"input_ids\"][0],\n",
                "    \"start_positions\": features[\"start_positions\"][0],\n",
                "    \"end_positions\": features[\"end_positions\"][0]\n",
                "}\n",
                "\n",
                "with open(\"data/preprocessed_sample.json\", \"w\") as f:\n",
                "    json.dump(sample_feature, f)\n",
                "\n",
                "print(\"Saved preprocessed sample to data/preprocessed_sample.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "localenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
